---
title: "Réseau de neurones"
author: "Francis Duval"
date: "2023-01-11"
output: rmdformats::material
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = T)
```

```{r, message = F}
library(targets)
library(tidyverse)
library(torch)
library(luz)
library(yardstick)
library(R6)
library(tidymodels)
library(glue)
library(rlang)
library(embed)
theme_set(theme_bw())
```

# Résumé

Le but ici est d'extraire des *features* à partir de l'information sur l'heure de la journée de la conduite. Chargeons le jeu de données qui contient cette information:

```{r}
JeuDonnees_train <- tar_read(JeuDonnees_train)
data_nn <- JeuDonnees_train$nn_data

names(data_nn)
```

Chaque véhicule est donc décrit par 27 variables. On a l'identifiant unique du véhicule (`vin`), l'indicatrice d'une réclamation (`claim_ind_cov_1_2_3_4_5_6`) ainsi que 24 variables nous informant quelle proportion de la conduite du véhicule est faite dans chaque heure de la journée (`t_1, t_2, ..., t_24`). Par exemple, voici la proportion de la conduite (en km) faite entre 08:00 et 09:00 par le premier véhicule de la base de données:

```{r}
data_nn[1, "t_9"]
```

Le premier véhicule a donc `r round(data_nn[1, "t_9"] * 100, 2)` % de sa conduite faite entre 08:00 et 09:00.

La variable `claim_ind_cov_1_2_3_4_5_6` nous dit si le véhicule a eu (ou non) une réclamation. On peut donc construire un réseau de neurones profond qui estime la probabilité de réclamer en utilisant les variables `t_1, ..., t_24`. Une fois le réseau entrainé (c'est-à-dire une fois les poids calculés), on peut extraire la dernière couche cachée pour chaque véhicule et utiliser ce vecteur (aussi appelé *embedding*) comme variable(s) dans une régression logistique. En effet, si on a la fonction sigmoide comme dernière fonction d'activation, le passage de la dernière couche cachée à la couche d'output est la même chose qu'une régression logistique. Utiliser un embedding (en contraste avec utiliser un réseau de neurones directement pour faire la classification) nous permet donc de profiter des avantages d'un GLM.

# Perceptron multicouches

Pour le réseau de neurones, j'utilise la librairie `torch`. Cette librairie utilise la classe `R6`, qui permet de programmer en paradigme « orienté objet » et qui ressemble beaucoup au paradigme de programmation en Python. On n'est plus dans le paradigme de programmation fonctionnelle auquel on est habitués en R.

## Préparation des données

Cette fonction (`time_of_day_dataset`) permet de définir une nouvelle classe appelée « time_of_day_dataset » et qui possède 3 méthodes (méthode = fonction encapsulée dans un objet). Elle est utile pour générer des jeux de données utilisables par `torch` en prenant en entrée un jeu de données « ordinaire » en R. La première méthode, `initialize`, permet de prétraiter le jeu de données. Dans ce cas-ci, on dit qu'on veut que notre matrice de design « x » soit constituée des variables `t_1, ..., t_24` et que la variable réponse « y » soit `claim_ind_cov_1_2_3_4_5_6`. On doit utiliser la fonction `torch_tensor` afin de transformer nos matrices/vecteurs en tenseurs puisque la librairie `torch` ne peut effectuer des calculs que sur des objects de classe `torch_tensor`. La méthode `.getitem` permet de sélectionner l'observation qu'on veut dans la base de données, tandis que `.length` permet de définir comment on veut mesure la « longeur » d'un objet de cette classe.

```{r}
time_of_day_dataset <- 
  dataset(
    name = "time_of_day_dataset",
    
    initialize = function(df) {
      data <- self$prepare_data(df)
      
      self$x_time <- data$x$time
      self$x_distance <- data$x$distance
      self$y <- data$y
    },
    
    .getitem = function(i) {
      list(x = list(time = self$x_time[i, ], distance = self$x_distance[i, ]), y = self$y[i, ])
    },
    
    .length = function() {
      self$y$size()[[1]]
    },
    
    prepare_data = function(df) {
      target_col <- 
        df$claim_ind_cov_1_2_3_4_5_6 %>% 
        as.integer() %>%
        `-`(1) %>%
        as.matrix()
      
      time_cols <- 
        df %>%
        select(starts_with("t_")) %>%
        as.matrix()
      
      distance_col <- 
        df %>% 
        mutate(dist_norm = (distance - mean(distance)) / sd(distance)) %>% 
        select(dist_norm) %>% 
        as.matrix()
      
      list(
        x = list(
          time = torch_tensor(time_cols), 
          distance = torch_tensor(distance_col)
        ),
        y = torch_tensor(target_col)
      )
    }
)
```

Ici, on définit les jeux de données train/validation/test. On a en tout environ 50 000 observations. Remarquer que le jeu de données `data_nn` ne contient que 40 000 observations. C'est parce que je me suis gardé 10 000 observations qui ne seront utilisées qu'à la toute fin du processus de modélisation afin d'éviter de faire du « data leakage ».

```{r}
train_indices <- 1:25000
valid_indices <- 25001:32500
test_indices <- 32501:40000

train_df <- data_nn[train_indices, ]
valid_df <- data_nn[valid_indices, ]
test_df <- data_nn[test_indices, ]
```

Ensuite, on crée nos 3 objets de classe `time_of_day_dataset`

```{r}
train_ds <- time_of_day_dataset(train_df)
valid_ds <- time_of_day_dataset(valid_df)
test_ds <- time_of_day_dataset(test_df)
```

pour ensuite créer nos 3 objets « dataloader » qui sont utiles pour entrainer le réseau de neurones en « batches ». J'ai choisi des batches de 128, ce qui veut dire qu'à chaque itération dans l'entrainement, le gradient est calculé sur la fonction de perte (entropie croisée binaire) pour 128 observations en même temps.

```{r}
set.seed(2023)
train_dl <- train_ds %>% dataloader(batch_size = 128, shuffle = F)
valid_dl <- valid_ds %>% dataloader(batch_size = 128, shuffle = F)
test_dl <- test_ds %>% dataloader(batch_size = 128, shuffle = F)
```

## Définition du perceptron

J'ai défini un perceptron assez de base pour commencer: on pourra subséquemment l'améliorer. On a 4 couches cachées et entre chacune, on applique la fonction d'activation ReLU (la plus classique de nos jours) élément par élément. Donc pour une seule observation,

-   la première couche cachée transforme le vecteur de dimension 24 en vecteur de dimension 32 (avec une matrice de poids 24 x 32 et un vecteur de biais de dimension 32);
-   la deuxième couche cachée transforme le vecteur de dimension 32 en vecteur de dimension 8 (avec une matrice de poids 32 x 8 et un vecteur de biais de dimension 8);
-   la troisième couche cachée transforme le vecteur de dimension 8 en vecteur de dimension 2 (avec une matrice de poids 8 x 2 et un vecteur de biais de dimension 2);
-   la quatrième couche cachée transforme le vecteur de dimension 2 en vecteur de dimension 1 (avec une matrice de poids 2 x 1 et un vecteur de biais de dimension 1);

Finalement, tout de suite après avoir appliqué la troisième couche, on applique la fonction sigmoide afin d'obtenir une probabilité entre 0 et 1. Noter qu'on a également ajouté une « skip connection » pour la distance totale conduite.

Ici, on définit la classe `classif_mlp`, qui permet de de créer des instances de réseaux de neurones avec l'architecture donnée. La méthode `initialize` permet de définir les couches, tandis que la méthode `forward` permet de produire l'output entre 0 et 1 avec l'input x, qui est un vecteur de dimension 24. Puisqu'on est intéressés à faire du « representation learning » (c'est-à-dire trouver une bonne représentation en moins de dimensions de notre vecteur d'input), j'ai aussi ajouté une méthode `extract_embedding`, qui permet d'accéder à la dernière couche cachée et donc de représenter l'input en 2 dimensions. Ce qu'on fait est donc en fait de la réduction de dimension, mais hautement non-linéaire.

```{r}
classif_mlp <- 
  nn_module(
    "classif_mlp",
    
    initialize = function() {
      self$linear1 = nn_linear(24, 32)
      self$linear2 = nn_linear(32, 8)
      self$linear3 = nn_linear(8, 2)
      self$linear4 = nn_linear(3, 1)
    },
    
    extract_embedding = function(x) {
      x$time %>%
        self$linear1() %>%
        nnf_relu() %>%
        self$linear2() %>%
        nnf_relu() %>%
        self$linear3() %>% 
        nnf_sigmoid()
    },

    forward = function(x) {
      emb <- self$extract_embedding(x)
      hl <- torch_cat(
        list(
          emb, 
          x$distance
        ), 
        dim = length(dim(emb))
      )

      hl %>%
        self$linear4() %>%
        nnf_sigmoid()
    }
)
```

Une fois qu'on a défini cette classe, il est possible de créer des instances qui appartiennent à celle-ci. Ça peut se faire en utilisant la méthode `new()`:

```{r}
modele <- classif_mlp$new()
modele
```

L'objet `modele` est un réseau de neurones avec l'architecture décrite plus haut. Cependant, il n'est pas entrainé encore: ses poids (ou paramètres) ont été initialisés au hasard, et il est donc totalement inutile pour classifier pour le moment. On peut accéder aux poids et aux biais de la troisième couche cachée avec:

```{r}
cat("poids: \n\n")
modele$parameters$linear3.weight
cat("\nbiais: \n\n")
modele$parameters$linear3.bias
```

Avec la méthode `forward`, il est possible de transformer l'input de dimension 24 en probabilité (donc en vecteur de dimension 1 entre 0 et 1):

```{r}
input <- train_ds[1]$x # On utilise la première observation du jeu d'entrainement
modele$forward(input)
```

Avec la méthode `extract_embedding`, il est possible de transformer l'input de dimension 24 en vecteur d'embedding de dimension 2:

```{r}
modele$extract_embedding(input)
```

## Entrainement du perceptron

Traditionnellement, en `torch`, on va programmer nous-mêmes la double boucle qui va permettre d'entrainer le réseau. C'est une boucle double parce qu'on itère sur toutes les batches du jeu de données et on passe plusieurs fois sur le jeu de données (epochs). On va donc à chaque itération faire une passe « forward » pour obtenir la valeur de la fonction de perte, faire la rétropropagation pour calculer le gradient et mettre à jour les paramètres avec l'optimiseur choisi.

Avec la librairie `luz`, cependant, plus besoin de faire la double boucle. On a qu'à d'abord utiliser la fonction `luz::setup` pour spécifier comment on veut entrainer et évaluer le modèle. On doit d'abord choisir la fonction de perte, qui est la plupart du temps l'entropie croisée binaire dans un contexte de classification binaire. On choisit ensuite l'optimiseur, qui correspond à la manière de mettre à jour les paramètres une fois le gradient calculé. Le plus simple est l'optimiseur « SGD » (stochastic gradient descent), qui va simplement mettre à jour les paramètres en soustrayant des paramètres courants le gradient multiplié par un taux d'apprentissage préalablement choisi. Ici, j'ai choisi « Adam », qui est très populaire et fonctionne habituellement mieux que le « SGD ». Finalement, on spécifie comment on veut évaluer la performance du modèle. Ici, j'ai choisi l'AUC. On va ensuite passer tout ça à la fonction `luz::fit`, qui va entrainer le modèle. On doit spécifier sur quel jeu de données on veut l'entrainer, sur quel jeu de données on veut l'évaluer et le nombre d'epochs (combien de fois on « passe » sur le jeu de données).


## TEST
```{r}
NeuralNet <- R6Class(
  
  classname = "NeuralNet",
  
  public =
    list(
      model_spec = NULL,
      train_ds = NULL,
      valid_ds = NULL,
      train_dl = NULL,
      valid_dl = NULL,
      fitted = NULL,
      
      
      
      initialize = function(model_spec, train_ds, valid_ds) {
        self$model_spec <- model_spec
        self$train_ds <- train_ds
        self$valid_ds <- valid_ds
        
        self$train_dl <- dataloader(train_ds, batch_size = 128, shuffle = F)
        self$valid_dl <- dataloader(valid_ds, batch_size = 128, shuffle = F)
      },
      
      fit = function(nb_epochs = 1) {
        self$fitted <- 
          self$model_spec %>%
          setup(
            loss = nnf_binary_cross_entropy,
            optimizer = optim_adam,
            metrics = list(luz_metric_binary_auroc(thresholds = seq(0, 0.5, by = 0.0005)), luz_metric_mse())
          ) %>%
          luz::fit(
            self$train_dl, 
            epochs = nb_epochs, 
            valid_data = self$valid_dl
          )
        invisible(self)
      },
      
      test = function(test_ds) {
        preds <- as.double(self$fitted$model$forward(test_ds[1:length(test_ds)]$x))
        preds_naif <- rep(mean(as.numeric(torch_cat(list(train_ds$y, valid_ds$y)))), length(test_ds))
        
        truth_numeric <- as.double(test_ds$y)
        truth_factor <- factor(truth_numeric, levels = c("1", "0"))
        
        mse = rmse_vec(truth = truth_numeric, estimate = preds) ^ 2
        mse_naif <- rmse_vec(truth = truth_numeric, estimate = preds_naif) ^ 2; print(mse_naif)
        
        list(
          auroc = roc_auc_vec(truth = truth_factor, estimate = preds),
          mse = mse,
          mse_skill = mse / mse_naif - 1
        )
      }
      
      
      

  )
)
```

```{r}
net <- NeuralNet$new(classif_mlp, train_ds, valid_ds)
net$fit()
net$test(test_ds)
```

### FIN TEST


```{r}
nb_epochs <- 2
set.seed(2023)
fit <- 
  classif_mlp %>%
  setup(
    loss = nnf_binary_cross_entropy,
    optimizer = optim_adam,
    metrics = list(luz_metric_binary_auroc(thresholds = seq(0, 0.5, by = 0.0005)), luz_metric_mse())
  ) %>%
  luz::fit(
    train_dl, 
    epochs = nb_epochs, 
    valid_data = valid_dl
  )
```

## Visualiser le processus d'entrainement

Une fois le réseau entrainé, on peut visualiser le processus d'entrainement. Les performances sur l'ensemble d'entrainement et de validation sont stockées dans l'objet `fit$records$metrics`:

```{r}
plot_training <- function(fit) {
  train_loss <- fit$records$metrics$train %>% map("loss") %>% flatten_dbl()
  train_auc <- fit$records$metrics$train %>% map("auc") %>% flatten_dbl()
  
  valid_loss <- fit$records$metrics$valid %>% map("loss") %>% flatten_dbl()
  valid_auc <- fit$records$metrics$valid %>% map("auc") %>% flatten_dbl()
  
  epochs <- seq_along(train_loss)
  
  dat <- 
    tibble(
      epochs,
      train_loss,
      train_auc,
      valid_loss,
      valid_auc
    ) %>% 
    pivot_longer(cols = -"epochs") %>% 
    separate(col = "name", into = c("dataset", "metric"), sep = "_")
  
  ggplot(dat, aes(x = epochs, y = value, col = dataset)) +
    geom_point(shape = 21) +
    geom_line() +
    scale_color_discrete(name = NULL) +
    ylab(NULL) +
    facet_grid(vars(metric), scales = "free_y")
}
```

```{r}
plot_training(fit)
```

On voit qu'on obtient une AUC de `r fit$records$metrics$valid %>% map("auc") %>% flatten_dbl() %>% tail(n=1)` après `r nb_epochs` épochs. Voici comment on pourrait améliorer les résultats:

- Plus d'épochs
- Plus de régularization (p. ex., dropout). On sait que les perceptrons complexes et bien régularisés tendent à bien performer.
- Batch normalization (normaliser les features avant chaque couche cachée).
- Calibration des hyperparamètres: learning rate (le plus important), fonction d'activation, nombre de neurones cachés, nombre de couches cachées.

## Performance sur l'ensemble test

On peut également tester le modèle sur l'ensemble test, avec:

```{r}
evaluation <- fit %>% evaluate(data = test_dl)
metrics <- get_metrics(evaluation)
print(evaluation)
```

Ou, de manière équivalente, avec:

```{r}
preds <- as.double(fit$model$forward(test_ds[1:7500]$x))
truth <- factor(as.double(test_ds$y), levels = c("1", "0"))
(res <- roc_auc_vec(truth = truth, estimate = preds))
```

On obtient donc une AUC de `r res` sur l'ensemble test.

## Extraire les embeddings

En entrainant un perceptron, le but est d'extraire des *features* utiles pour prédire les réclamations à partir des vecteurs de dimension 24 qui caractérisent chacun des véhicules. Ces features sont aussi appelées des *embeddings* et correspondent à une représentation en moins de dimensions du vecteur de dimension 24. Ici, la dernière couche cachée a 2 neurones, donc notre embedding sera en 2 dimensions. Pour obtenir cet embedding, on doit « couper la tête » du réseau, ce que j'ai fait avec la méthode `extract_embedding` de la classe `classif_mlp`. Pour obtenir l'embedding d'un véhicule donné, on va simplement prendre son vecteur x (dimension 24) et le passer dans la méthode `extract_embedding` du perceptron entrainé. Par exemple, on peut obtenir le embedding du premier véhicule avec:

```{r}
(emb_premier_veh <- fit$model$extract_embedding(train_ds[1]$x))
```
On va maintenant obtenir les embeddings pour tous les véhicules des 3 bases train, valid et test:

```{r}
emb_train <- fit$model$extract_embedding(train_ds[1:25000]$x) %>% as.matrix() %>% as_tibble()
emb_valid <- fit$model$extract_embedding(valid_ds[1:7500]$x) %>% as.matrix() %>% as_tibble()
emb_test <- fit$model$extract_embedding(test_ds[1:7500]$x) %>% as.matrix() %>% as_tibble()
```

```{r}
glimpse(emb_train)
```
On a créé des embeddings en 2 dimensions, ce qui veut dire qu'on peut les visualiser avec un nuage de points:

```{r}
ggplot(emb_train, aes(x = V1, y = V2)) + geom_point(alpha = 0.1, size = 0.8)
```

On voit que les 2 composantes sont fortement liées. C'est probablement signe qu'il faut changer l'architecture du réseau. Idéalement, on veut que les représentations apprises dans la dernière couche cachée soient indépendantes. Ça veut peut-être dire que tout est appris trop « tôt » dans le perceptron.


# Régression logistique

Il est maintenant l'heure de tester nos embeddings dans une régression logistique. Ajoutons d'abord la variable réponse aux 3 jeux de données d'embeddings. 

```{r}
train <- emb_train %>% bind_cols(train_df["claim_ind_cov_1_2_3_4_5_6"])
valid <- emb_valid %>% bind_cols(valid_df["claim_ind_cov_1_2_3_4_5_6"])
test <- emb_test %>% bind_cols(test_df["claim_ind_cov_1_2_3_4_5_6"])
```

## Seulement les embeddings

Testons une régression logistique avec seulement les embeddings comme prédicteurs. On entraine d'abord sur l'ensemble `train`:

```{r}
glm_fit <- glm(claim_ind_cov_1_2_3_4_5_6 ~ ., family = binomial, data = train)
```

Et on évalue sur les ensemble `valid` et `test`:

```{r}
glm_preds_valid <- predict(glm_fit, newdata = valid, type = "response")
glm_preds_test <- predict(glm_fit, newdata = test, type = "response")
```

**AUROC**
```{r}
truth_valid <- factor(valid$claim_ind_cov_1_2_3_4_5_6, levels = c("1", "0"))
truth_test <- factor(test$claim_ind_cov_1_2_3_4_5_6, levels = c("1", "0"))

roc_auc_vec(truth_valid, glm_preds_valid) # AUC valid
roc_auc_vec(truth_test, glm_preds_test) # AUC test
```

**Brier score**


## GLMNET embeddings + variables classiques + distance

Testons maintenant une régression glmnet avec les embeddings en plus des variables classiques + distance. Créons-nous d'abord une classe R6 pour faciliter la calibration et l'entrainement des modèles elastic-net:

```{r}
ClassifElasNet <- R6Class(
  
  classname = "ClassifElasNet",
  
  public =
    list(
      data = NULL,
      response = NULL,
      tune_res = NULL,
      best_params = NULL,
      final_wf = NULL,
      fitted = NULL,
      
      initialize = function(data, response) {
        self$data <- data
        self$response <- response
      },
      
      tune = function() {
        tune_spec <- logistic_reg(engine = "glmnet", penalty = tune(), mixture = tune())
        resamples <- vfold_cv(self$data, v = 4, strata = all_of(self$response))
        formula <- as.formula(glue("{self$response} ~ ."))
        grid <- grid_regular(penalty(), mixture(), levels = c(50, 5))
        recipe <-
          recipe(formula, data = self$data) %>%
          step_other(all_nominal_predictors(), threshold = 0.05) %>%
          step_lencode_glm(all_nominal_predictors(), outcome = self$response) %>%
          step_impute_bag(all_predictors()) %>%
          step_YeoJohnson(all_numeric_predictors()) %>% 
          step_normalize(all_numeric_predictors())
        
        wf <-
          workflow() %>%
          add_model(tune_spec) %>%
          add_recipe(recipe)
        
        tuning <- 
          tune_grid(
            wf,
            resamples = resamples,
            grid = grid,
            metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
            control = control_grid(save_pred = F)
          )
        
        self$tune_res <- tuning
        self$best_params <- tuning %>% select_best(metric = "roc_auc") %>% select(penalty, mixture)
        self$final_wf <- wf %>% finalize_workflow(self$best_params)
        invisible(self)
      },
      
      fit = function() {
        self$fitted <- parsnip::fit(self$final_wf, data = self$data)
        invisible(self)
      },
      
      plot_coefs = function() {
        df <- 
          self$fitted %>% 
          extract_fit_parsnip() %>% 
          tidy() %>% 
          filter(term != "(Intercept)")
        
        df_non_zero <- 
          df %>% 
          filter(estimate != 0)
        
        nb_zero_coef <- nrow(df) - nrow(df_non_zero)
    
        df_non_zero %>% 
          mutate(Sign = if_else(estimate > 0, "+", "-")) %>% 
          mutate(abs_estimate = abs(estimate)) %>% 
          mutate(term = fct_reorder(term, abs_estimate)) %>% 
          ggplot(aes(x = term, y = abs_estimate, fill = Sign)) +
          geom_col(alpha = 0.7) +
          xlab(NULL) +
          ylab("Absolute value of coefficient") +
          scale_fill_manual(values = c("#a61d21", "#00743F")) +
          coord_flip() +
          labs(caption = glue("Note: {nb_zero_coef} other coefficients are zero"))
      },
      
      get_preds = function(data) {
        predict(self$fitted, new_data = data, type = "prob")$.pred_1
      },
      
      get_auc = function(data) {
        truth <- data[[self$response]]
        preds <- self$get_preds(data)
        roc_auc_vec(truth, 1 - preds)
      },
      
      print = function() {
        cat("Régression logistique avec régularisation elastic-net \n\n")
        cat("\t", nrow(self$data), " observations\n\n", sep = "")
        cat("\tVariable réponse: ", self$response, "\n\n", sep = "")
        cat("\tPrédicteurs:\n", glue("{names(select(self$data, -self$response))}\n"), sep = "\n\t")
        invisible(self)
      }
  )
)
```

On va ensuite se créer des jeux de données d'entrainement et de test avec et sans les embeddings:

```{r}
tele_contract_train <- 
  tar_read(tele_contract_train) %>% 
  group_by(vin) %>% 
  slice(1) %>% 
  ungroup()
```

```{r}
data_class <- select(tele_contract_train, expo:years_licensed, claim_ind_cov_1_2_3_4_5_6)

train_class <- slice(data_class, train_indices)
test_class <- slice(data_class, c(valid_indices, test_indices))

train_class_emb <- bind_cols(train_class, emb_train)
test_class_emb <- bind_cols(test_class, bind_rows(emb_valid, emb_test))
```


### Benchmark classique + distance

Testons d'abord un GLMNET avec seulement les variables classiques + distance pour avoir un point de comparaison. On rentre la distance dans la définition de « variables classiques ».

On se définit un objet de classe `ClassifElasNet` avec le jeu de données d'entrainement SANS embeddings:

```{r}
en_class <- ClassifElasNet$new(data = train_class, response = "claim_ind_cov_1_2_3_4_5_6")
en_class
```

Ensuite, on calibre les hyperparamètres et on entraine le modèle:

```{r}
en_class$tune()
en_class$fit()
```

Voici les meilleurs hyperparamètres trouvés:

```{r}
en_class$best_params
```

Voici les coefficients calculés:

```{r}
en_class$plot_coefs()
```

On calcule l'AUC sur l'ensemble test:

```{r}
en_class$get_auc(test_class)
```


### Modèle classique + distance + embeddings

On se définit un objet de classe `ClassifElasNet` avec le jeu de données d'entrainement AVEC embeddings:

```{r}
en_class_emb <- ClassifElasNet$new(data = train_class_emb, response = "claim_ind_cov_1_2_3_4_5_6")
en_class_emb
```

Ensuite, on calibre les hyperparamètres et on entraine le modèle:

```{r}
en_class_emb$tune()
en_class_emb$fit()
```

Voici les meilleurs hyperparamètres trouvés:

```{r}
en_class_emb$best_params
```

Voici les coefficients calculés:

```{r}
en_class_emb$plot_coefs()
```

On calcule l'AUC sur l'ensemble test:

```{r}
en_class_emb$get_auc(test_class_emb)
```

Les embeddings créés offrent donc une amélioration de l'AUC de `r en_class_emb$get_auc(test_class_emb) - en_class$get_auc(test_class)` sur l'ensemble test. Il faut se souvenir que les embeddings ont été calculés avec l'heure de la journée des trajets uniquement comme information. Il nous resterait la vitesse moyenne, la vitesse maximale, la distance, la durée, et le jour de la semaine à utiliser. Aussi, le réseau de neurones utilisé est très basique et pourrait probablement être amélioré. Finalement, on pourrait peut-être bénéficier d'un embedding en plus de 2 dimensions. Mais 2 dimensions est ici très pratique car on peut le visualiser.

## GLMNET - variables télématiques créées à la main

```{r}
dat_tele <- select(tele_contract_train, avg_daily_distance:claim_ind_cov_1_2_3_4_5_6)

train_tele <- slice(dat_tele, train_indices)
test_tele <- slice(dat_tele, c(valid_indices, test_indices))
```

```{r}
en_tele <- ClassifElasNet$new(data = train_tele, response = "claim_ind_cov_1_2_3_4_5_6")
en_tele
```
```{r}
en_tele$tune()
en_tele$fit()
```

```{r}
en_tele$plot_coefs()
```

```{r}
en_tele$get_auc(test_tele)
```


## GLMNET - variables classiques + télématiques créées à la main

```{r}
dat_tele_class <- select(tele_contract_train, expo:claim_ind_cov_1_2_3_4_5_6)

train_tele_class <- slice(dat_tele_class, train_indices)
test_tele_class <- slice(dat_tele_class, c(valid_indices, test_indices))
```

```{r}
en_tele_class <- ClassifElasNet$new(data = train_tele_class, response = "claim_ind_cov_1_2_3_4_5_6")
en_tele_class
```

```{r}
en_tele_class$tune()
en_tele_class$fit()
```

```{r}
en_tele_class$plot_coefs()
```

```{r}
en_tele_class$get_auc(test_tele_class)
```

